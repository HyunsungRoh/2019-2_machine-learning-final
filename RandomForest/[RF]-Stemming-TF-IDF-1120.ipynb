{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Data Loading\n",
    "\n",
    "프로젝트 폴더에서 데이터를 불러온다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "\n",
    "\n",
    "dataOutPath='./data_out/'\n",
    "File = 'train_all.csv'\n",
    "#testFile = 'FDOT-clean.csv'\n",
    "\n",
    "Input = pd.read_csv(dataOutPath+File)\n",
    "#testInput = pd.read_csv(dataOutPath+testFile)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Input['clauses'][:414]\n",
    "test_label = Input['label'][:414]\n",
    "\n",
    "train_data = Input['clauses'][415:]\n",
    "train_label = Input['label'][415:]\n",
    "\n",
    "train_data_df = pd.DataFrame(data = {\"clauses\": train_data, \"label\":train_label})\n",
    "train_data_df.to_csv(dataOutPath+'train_data.csv')\n",
    "\n",
    "test_data_df = pd.DataFrame(data = {\"clauses\": test_data, \"label\":test_label})\n",
    "test_data_df.to_csv(dataOutPath+'test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorizing\n",
    "\n",
    "- ngram_range: 단어 빈도의 기본단위를 얼만큼의 n-gram으로 설정할지 \n",
    "- min_df : 설정한 값보다 특정 토큰의 df 값이 더 작게 나오면 벡터화에서 제거 \n",
    "- analyzer: char / word - 임베딩 단위를 문자로할지 단어로할지. \n",
    "- sublinear_tf: 문서의 단어 빈도수에 대한 스무딩(smoothing)을 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000,\n",
    "                             ngram_range=(1,1),\n",
    "                            min_df=0.0,\n",
    "                            analyzer=\"word\",\n",
    "                            sublinear_tf =True)\n",
    "\n",
    "train_vec=vectorizer.fit_transform(train_data)\n",
    "test_vec = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1659x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19401 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 훈련데이터와 검증데이터 나누기\n",
    "\n",
    "전체 훈련데이터에 사용되는 문장 4049개를  t_size 비율만큼 훈련:검증 데이터로 나눈다. \n",
    "sklearn의 train_test_split 함수를 사용해서 자동으로 분할한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "t_size=0.2 \n",
    "r_seed=42\n",
    "\n",
    "train_cl, eval_cl, train_lb, eval_lb = train_test_split(train_vec, train_label, test_size=t_size, random_state=r_seed )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 랜덤포레스트 분류\n",
    "\n",
    "- RandomForestClassifier 는 sklearn의 ensemble  라이브러리에서 제공한다. \n",
    "- forest라는 이름으로 분류기 객체를 생성하고, n_estimators 변수로 결정트리의 개수를 지정한다. \n",
    "- forest.fit 함수의 인자로 훈련 데이터와 레이블을 주입한다. \n",
    "- 검증데이터셋 (eval_cl, eval_lb)로 분류정확도를 측정한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "forest.fit(train_cl, train_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of training: 1.000000\n",
      "Accuracy: 0.915663\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of training: %f\" % forest.score(train_cl, train_lb))\n",
    "print(\"Accuracy: %f\" % forest.score(eval_cl, eval_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 분류결과 \n",
    "- n_estimators =n 일때, n 값이 커질수록 정확도가 향상됨을 알 수 있다. \n",
    "- n=100 일때 accuracy = 95% 달성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# 테스트 데이터 파일에 쓰기 -\n",
    "\n",
    "DATA_OUT_PATH='./data_out/results/'\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "# 위에서 만든 랜덤 포레스트 분류기를 통해 예측값을 가져온다.\n",
    "result = forest.predict(test_vec)\n",
    "\n",
    "# 판다스 데이터 프레임을 통해 데이터를 구성해서 output에 넣는다.\n",
    "output = pd.DataFrame( data={\"label\":test_label,  \"predict\": result} )\n",
    "\n",
    "# 이제 csv파일로 만든다.\n",
    "output.to_csv(DATA_OUT_PATH + \"tfidf-predict-test.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
