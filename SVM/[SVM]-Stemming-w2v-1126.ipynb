{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine with Word2Vec (+stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 처리에 필요한 백그라운드를 받아온다\n",
    "- os: 운영체제 인터페이스\n",
    "- re: 정규 표현식 라이브러리\n",
    "- pandas, numpy: 데이터 분석 및 배열 계산에 필요한 라이브러리\n",
    "- nltk: 자연어 처리에 필요한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')  # 오류가 없는 코드에 오류가 있다고 발생하는 메세지 차단\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  데이터 경로와 훈련/테스트 데이터 할당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input / output 데이터가 저장되는 경로를 우선 지정해주고, 어떠한 데이터가 훈련 / 테스트에 사용될지 선언해준다.\n",
    "또한 후반부에 사용될 훈련/검증셋 나누는 비율(eval_split), 데이터를 섞는 횟수(random_seed)의 수를 정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_path = './data_in/'\n",
    "data_out_path = './data_out/'\n",
    "train_data = 'train_data.csv'\n",
    "test_data = 'test_data.csv'\n",
    "\n",
    "train_data = pd.read_csv(data_in_path + train_data)\n",
    "test_data = pd.read_csv(data_in_path + test_data)\n",
    "\n",
    "random_seed = 10\n",
    "eval_split = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변수 지정 + 문장을 단어로 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clause = list(train_data['clauses'])\n",
    "train_label = list(train_data['label'])\n",
    "\n",
    "#print(train_clause[0])\n",
    "#print(train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for clauses in train_clause:\n",
    "    sentences.append(clauses.split())\n",
    "    \n",
    "#print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split된 문장을 단어 단위로 Vectorizing(Word2Vec) 해준다. 그 후, 모델을 구축한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_features: 단어 벡터의 크기\n",
    "- min_word_count: 빈도수가 적은 단어는 무시\n",
    "- num_workers: 사용되는 프로세스 개수\n",
    "- context: 한 단어 앞,뒤로 보고 싶은 단어 개수\n",
    "- downsample: 높은 빈도의 단어가 다운샘플링 되게 하기위한 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 100\n",
    "min_word_count = 40\n",
    "num_workers = 6\n",
    "context = 4\n",
    "downsample = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-27 15:44:41,553 : INFO : collecting all words and their counts\n",
      "2019-11-27 15:44:41,554 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-27 15:44:41,559 : INFO : collected 2139 word types from a corpus of 24305 raw words and 1659 sentences\n",
      "2019-11-27 15:44:41,560 : INFO : Loading a fresh vocabulary\n",
      "2019-11-27 15:44:41,562 : INFO : effective_min_count=40 retains 131 unique words (6% of original 2139, drops 2008)\n",
      "2019-11-27 15:44:41,563 : INFO : effective_min_count=40 leaves 12552 word corpus (51% of original 24305, drops 11753)\n",
      "2019-11-27 15:44:41,564 : INFO : deleting the raw counts dictionary of 2139 items\n",
      "2019-11-27 15:44:41,565 : INFO : sample=0.001 downsamples 131 most-common words\n",
      "2019-11-27 15:44:41,566 : INFO : downsampling leaves estimated 5907 word corpus (47.1% of prior 12552)\n",
      "2019-11-27 15:44:41,567 : INFO : estimated required memory for 131 words and 100 dimensions: 170300 bytes\n",
      "2019-11-27 15:44:41,568 : INFO : resetting layer weights\n",
      "2019-11-27 15:44:41,594 : INFO : training model with 6 workers on 131 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-11-27 15:44:41,604 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-27 15:44:41,607 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-27 15:44:41,609 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-27 15:44:41,610 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-27 15:44:41,611 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-27 15:44:41,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-27 15:44:41,612 : INFO : EPOCH - 1 : training on 24305 raw words (5807 effective words) took 0.0s, 561459 effective words/s\n",
      "2019-11-27 15:44:41,620 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-27 15:44:41,622 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-27 15:44:41,624 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-27 15:44:41,624 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-27 15:44:41,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-27 15:44:41,626 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-27 15:44:41,626 : INFO : EPOCH - 2 : training on 24305 raw words (5900 effective words) took 0.0s, 691539 effective words/s\n",
      "2019-11-27 15:44:41,635 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-27 15:44:41,636 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-27 15:44:41,638 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-27 15:44:41,639 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-27 15:44:41,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-27 15:44:41,641 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-27 15:44:41,642 : INFO : EPOCH - 3 : training on 24305 raw words (5892 effective words) took 0.0s, 839783 effective words/s\n",
      "2019-11-27 15:44:41,651 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-27 15:44:41,653 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-27 15:44:41,654 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-27 15:44:41,654 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-27 15:44:41,655 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-27 15:44:41,656 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-27 15:44:41,656 : INFO : EPOCH - 4 : training on 24305 raw words (5858 effective words) took 0.0s, 685981 effective words/s\n",
      "2019-11-27 15:44:41,664 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-27 15:44:41,666 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-27 15:44:41,666 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-27 15:44:41,667 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-27 15:44:41,668 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-27 15:44:41,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-27 15:44:41,669 : INFO : EPOCH - 5 : training on 24305 raw words (5881 effective words) took 0.0s, 736672 effective words/s\n",
      "2019-11-27 15:44:41,669 : INFO : training on a 121525 raw words (29338 effective words) took 0.1s, 394600 effective words/s\n",
      "2019-11-27 15:44:41,670 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "           size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-27 15:44:42,010 : INFO : saving Word2Vec object under ./data_out/1126-stem_word2vec_model, separately None\n",
      "2019-11-27 15:44:42,011 : INFO : not storing attribute vectors_norm\n",
      "2019-11-27 15:44:42,011 : INFO : not storing attribute cum_table\n",
      "2019-11-27 15:44:42,014 : INFO : saved ./data_out/1126-stem_word2vec_model\n"
     ]
    }
   ],
   "source": [
    "model_name = '1126-stem_word2vec_model'\n",
    "model.save(data_out_path+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features),dtype=np.float64)\n",
    "\n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(clauses, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    for s in clauses:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "\n",
    "    clauseFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return clauseFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsfra\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련데이터의 문장과 라벨을 훈련, 검증 셋으로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "vec = train_data_vecs\n",
    "lab = np.array(train_label)\n",
    "\n",
    "vec = np.nan_to_num(vec)\n",
    "\n",
    "vec_train, vec_eval, lab_train, lab_eval = train_test_split(vec, lab, test_size=eval_split, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vec_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서포트 벡터 머신을 import해와서 변수를 지정한 후 훈련을 시작한다.\n",
    "\n",
    "- kernel: 커널의 종류\n",
    "- C: error term값을 정하는 변수\n",
    "- class_weight: 학습 데이터의 균형\n",
    "- random state: 데이터 셔플 횟수\n",
    "- probability: 확률 추정이 가능한지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=3.0, cache_size=300, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=True, random_state=10,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel = 'linear', C = 3.0, random_state = random_seed, probability = True, class_weight = 'balanced', \n",
    "              cache_size = 300)\n",
    "\n",
    "clf.fit(vec_train, lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of training: 0.607385\n",
      "Accuracy: 0.578313\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of training: %f\" % clf.score(vec_train, lab_train))\n",
    "print(\"Accuracy: %f\" % clf.score(vec_eval, lab_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기서부터 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clause = list(test_data['clauses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clauses = []\n",
    "\n",
    "for clauses in test_clause:\n",
    "    test_clauses.append(clauses.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsfra\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(test_clauses, model, num_features)\n",
    "test_vecs = np.nan_to_num(test_data_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 위에서 만든 랜덤 포레스트 분류기를 통해 예측값을 가져온다.\n",
    "result = clf.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 테스트 데이터 파일에 쓰기 -\n",
    "\n",
    "test_clauses = list(test_data['clauses'])\n",
    "test_label = list(test_data['label'])\n",
    "\n",
    "if not os.path.exists(data_out_path):\n",
    "    os.makedirs(data_out_path)\n",
    "\n",
    "# 판다스 데이터 프레임을 통해 데이터를 구성해서 output에 넣는다.\n",
    "output = pd.DataFrame( data={\"label\": test_label ,  \"predict\": result} )\n",
    "\n",
    "# 이제 csv파일로 만든다.\n",
    "output.to_csv(data_out_path + \"svm-w2v-predict-test-stem.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Accuracy: 0.589372\n",
      "Precision: 0.619048\n",
      "Recall: 0.719149\n",
      "F1-Score: 0.665354\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(lab_eval, (clf.predict_proba(vec_eval)[:, 1]))\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print(\"------------\")\n",
    "print(\"Accuracy: %f\" % clf.score(test_vecs, test_data['label']))  #checking the accuracy\n",
    "print(\"Precision: %f\" % metrics.precision_score(test_data['label'], result))\n",
    "print(\"Recall: %f\" % metrics.recall_score(test_data['label'], result))\n",
    "print(\"F1-Score: %f\" % metrics.f1_score(test_data['label'], result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
